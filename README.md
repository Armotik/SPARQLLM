## Overview

This repository provides a SPARQL execution layer extended with user defined functions (UDFs) that can call local resources (files, directories, CSV/HTML content) and, optionally, external search, vector, or large language model backends. The design goal is to let a single SPARQL query orchestrate multi‑step data acquisition and light transformation without writing glue code.

This README focuses on running a first query without any API key or remote dependency. Optional advanced capabilities (web search, online LLMs, vector similarity) can be enabled later but are not required for the basic examples below.

## Key Features (short list)
* Extend SPARQL with simple function calls (custom prefixes) returning named graphs.
* Read local files and directories during query execution.
* Produce intermediate named graphs and re‑query them in the same SPARQL request.
* Keep / replay gathered data (optional) for deterministic reruns offline.

## Quick Start (offline only)

Clone and install (Python 3.10+ recommended):
```
git clone <your-fork-or-clone-url>
cd SPARQLLM
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install .
```

Verify the command‑line tool is available:
```
slm-run --help
```

## Minimal Configuration

The file `config.ini` already contains default sections. No API keys are needed for local file queries. You can proceed directly to execution.

## First Example: Read a Local CSV

Run a SPARQL query that loads and inspects a small CSV file (see `queries/simple-csv.sparql`):
```
slm-run --config config.ini -f queries/filesystem/simple-csv.sparql --debug
```
Use `--debug` to see which custom functions are registered and how graphs are materialized.

### Read a Single File
```
slm-run --config config.ini -f queries/filesystem/readfile.sparql --debug
```

### Iterate Over a Directory
```
slm-run --config config.ini -f queries/ReadDir.sparql --debug
```

These examples prove the local GGFs (file and directory access) are working with no external services.

## Inspecting Results
By default, query solutions stream to stdout. To persist the result bindings (one row per line) use:
```
slm-run --config config.ini -f queries/readfile.sparql -o result.txt
```

To keep all intermediate named graphs for later offline replay:
```
slm-run --config config.ini -f queries/readfile.sparql --keep-store session.nq
```
Then rerun the logic (adding new clauses or different projection) without touching the network:
```
slm-run --config config.ini --load session.nq --format nquads -q "SELECT (COUNT(*) AS ?c) WHERE { ?s ?p ?o }"
```

## Optional: Local Search / Vector (No API Keys)
If you want local text search or approximate vector similarity you can build indices. These steps are optional and can be skipped for a first run.

1. Build a Whoosh index (keyword search):
```
slm-index-whoosh
```
2. Build a FAISS index (vector embeddings):
```
slm-index-faiss
```
3. Test:
```
slm-search-whoosh --help
slm-search-faiss --help
```

Queries that combine Wikidata (remote public SPARQL endpoint) with local search exist under `queries/` (e.g., `city-search.sparql`). These do not require private API keys but do rely on the public Wikidata endpoint being reachable.

## Optional: Local LLM via Ollama (No API Key)
If you have [Ollama](https://ollama.com/) installed you can enable local LLM generation without any API key:
```
curl -fsSL https://ollama.com/install.sh | sh
ollama serve &
ollama pull llama3.1:latest
```
Then adapt a query calling the local LLM function variants (see queries containing `LLMGRAPH_OLLA`). This is entirely optional.

## Advanced (Requires External Keys – Omitted by Default)
The codebase also supports web search APIs and hosted LLM providers (e.g., OpenAI, Groq, Mistral). These require setting environment variables and adjusting `[Requests]` in `config.ini`. Since the goal here is a key‑less onboarding path, detailed instructions are intentionally omitted. You can discover the expected variable names by searching for provider modules in `SPARQLLM/udf/`.

## Running With Debug Logs
Use `--debug` to activate verbose logging (registration of custom functions, external call timing, provenance enrichment). This is helpful when authoring new queries or diagnosing performance.

## Keeping Things Deterministic
For reproducibility across environments:
* Pin dependencies in `requirements.txt` (already present).
* Use `--keep-store` to freeze retrieved graphs.
* Rerun with `--load` to avoid external calls.

## Query Authoring Patterns
Custom functions typically bind a named graph that you immediately re‑enter:
```
BIND(ex:SLM-READFILE(?path) AS ?g)
GRAPH ?g { ?doc ?p ?o }
```
This pattern lets you chain multiple stages (e.g., directory listing → file read → lightweight extraction) inside one SPARQL query.

Some aliases may hide verbose JSON argument objects. When available they appear as simple function calls (e.g., `ggf:SEARCH("term")`). Inspect existing queries for concrete usage.

## Testing
Run the test suite (fast, mostly local):
```
pytest -q
```

## Troubleshooting
| Symptom | Likely Cause | Quick Fix |
| ------- | ------------ | --------- |
| Command `slm-run` not found | Package not installed in current venv | Re‑activate venv, `pip install .` |
| Empty query results | Projection variables not bound | Add a temporary `SELECT *` to inspect bindings |
| Slow run | Remote endpoint (e.g., Wikidata) latency | Test with a local file query first |
| Function URI collision | Duplicate alias registration | Adjust alias name or guard registration |

## Contributing (Neutral Guidelines)
1. Keep new UDFs minimal; return a coherent named graph.
2. Add a focused test where practical.
3. Avoid introducing hard API key dependencies in core logic.
4. Prefer pure Python standard library unless a dependency adds clear value.

## License
Refer to the repository’s license file (if present) for usage terms. In absence of an explicit license, treat the code as “all rights reserved” until clarified.

---
You now have everything needed to execute a first SPARQL query locally with no API keys. Explore `queries/` and iterate from there.